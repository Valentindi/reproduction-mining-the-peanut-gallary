{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import enumerate, len\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [ \"../data/df_test_1.csv\", \"../data/df_test_2.csv\"]\n",
    "ngrams = [2,3]\n",
    "repl_prod_name = [True, False]\n",
    "tokens = [\"common_tokens\", \"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    train_idxes = df.test_idx.unique()\n",
    "    print(train_idxes)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for train_idx in train_idxes:\n",
    "        mask = df.test_idx == train_idx\n",
    "\n",
    "        df_train = pd.read_csv(path)\n",
    "        df_test = copy(df_train)\n",
    "\n",
    "        df_train = df_train[~mask]\n",
    "        df_test = df_test[mask]\n",
    "\n",
    "        res += [(df_train, df_test)]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(dataset_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5792 189 5792 189\n",
      "4591 1390 4591 1390\n",
      "4960 1021 4960 1021\n",
      "4320 1661 4320 1661\n",
      "5283 698 5283 698\n",
      "5764 217 5764 217\n",
      "5176 805 5176 805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 145 tn= 19 fp= 14 fn= 11\n",
      "scoring= 0.8677248677248677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 1165 tn= 86 fp= 123 fn= 16\n",
      "scoring= 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 813 tn= 35 fp= 152 fn= 21\n",
      "scoring= 0.8305582761998042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 1386 tn= 85 fp= 141 fn= 49\n",
      "scoring= 0.8856110776640578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 561 tn= 43 fp= 76 fn= 18\n",
      "scoring= 0.8653295128939829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 183 tn= 9 fp= 10 fn= 15\n",
      "scoring= 0.8847926267281107\n",
      "tp= 618 tn= 68 fp= 112 fn= 7\n",
      "scoring= 0.8521739130434782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 4871 tn= 345 fp= 628 fn= 137\n",
      "scoring= 0.8720949673967564\n"
     ]
    }
   ],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 145 tn= 28 fp= 5 fn= 11\n",
      "scoring= 0.9153439153439153\n",
      "tp= 1161 tn= 143 fp= 66 fn= 20\n",
      "scoring= 0.9381294964028777\n",
      "tp= 806 tn= 88 fp= 99 fn= 28\n",
      "scoring= 0.8756121449559255\n",
      "tp= 1361 tn= 172 fp= 54 fn= 74\n",
      "scoring= 0.9229379891631547\n",
      "tp= 561 tn= 92 fp= 27 fn= 18\n",
      "scoring= 0.9355300859598854\n",
      "tp= 193 tn= 12 fp= 7 fn= 5\n",
      "scoring= 0.9447004608294931\n",
      "tp= 617 tn= 106 fp= 74 fn= 8\n",
      "scoring= 0.8981366459627329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 4844 tn= 641 fp= 332 fn= 164\n",
      "scoring= 0.9170707239592042\n"
     ]
    }
   ],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Witten Bell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_witten_bell(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        \n",
    "        if it %500 == 0:\n",
    "            print(it, x.shape)\n",
    "        N = sum(row)\n",
    "        #print(\"N\", N)\n",
    "        M = sum([1 for x in row if x != 0])\n",
    "        #print(\"M\", M)\n",
    "        row = row/row.sum()\n",
    "        nval = 1/(N+M)\n",
    "        #print(\"applying\", nval)\n",
    "        row = row.replace(0, nval)\n",
    "        #print(\"set value\")\n",
    "        res.append(row)\n",
    "        #print(\"ready with row\")\n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "iterating\n",
      "0 (5792, 9137)\n",
      "500 (5792, 9137)\n",
      "1000 (5792, 9137)\n",
      "1500 (5792, 9137)\n",
      "2000 (5792, 9137)\n",
      "2500 (5792, 9137)\n",
      "3000 (5792, 9137)\n",
      "3500 (5792, 9137)\n",
      "4000 (5792, 9137)\n",
      "4500 (5792, 9137)\n",
      "5000 (5792, 9137)\n",
      "5500 (5792, 9137)\n",
      "start\n",
      "iterating\n",
      "0 (189, 9137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 156 tn= 0 fp= 33 fn= 0\n",
      "scoring= 0.8253968253968254\n",
      "start\n",
      "iterating\n",
      "0 (4591, 7935)\n",
      "500 (4591, 7935)\n",
      "1000 (4591, 7935)\n",
      "1500 (4591, 7935)\n",
      "2000 (4591, 7935)\n",
      "2500 (4591, 7935)\n",
      "3000 (4591, 7935)\n",
      "3500 (4591, 7935)\n",
      "4000 (4591, 7935)\n",
      "4500 (4591, 7935)\n",
      "start\n",
      "iterating\n",
      "0 (1390, 7935)\n",
      "500 (1390, 7935)\n",
      "1000 (1390, 7935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 1181 tn= 0 fp= 209 fn= 0\n",
      "scoring= 0.8496402877697842\n",
      "start\n",
      "iterating\n",
      "0 (4960, 8068)\n",
      "500 (4960, 8068)\n",
      "1000 (4960, 8068)\n",
      "1500 (4960, 8068)\n",
      "2000 (4960, 8068)\n",
      "2500 (4960, 8068)\n",
      "3000 (4960, 8068)\n",
      "3500 (4960, 8068)\n",
      "4000 (4960, 8068)\n",
      "4500 (4960, 8068)\n",
      "start\n",
      "iterating\n",
      "0 (1021, 8068)\n",
      "500 (1021, 8068)\n",
      "1000 (1021, 8068)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 834 tn= 0 fp= 187 fn= 0\n",
      "scoring= 0.8168462291870715\n",
      "start\n",
      "iterating\n",
      "0 (4320, 7986)\n",
      "500 (4320, 7986)\n",
      "1000 (4320, 7986)\n",
      "1500 (4320, 7986)\n",
      "2000 (4320, 7986)\n",
      "2500 (4320, 7986)\n",
      "3000 (4320, 7986)\n",
      "3500 (4320, 7986)\n",
      "4000 (4320, 7986)\n",
      "start\n",
      "iterating\n",
      "0 (1661, 7986)\n",
      "500 (1661, 7986)\n",
      "1000 (1661, 7986)\n",
      "1500 (1661, 7986)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 1435 tn= 0 fp= 226 fn= 0\n",
      "scoring= 0.863937387116195\n",
      "start\n",
      "iterating\n",
      "0 (5283, 8299)\n",
      "500 (5283, 8299)\n",
      "1000 (5283, 8299)\n",
      "1500 (5283, 8299)\n",
      "2000 (5283, 8299)\n",
      "2500 (5283, 8299)\n",
      "3000 (5283, 8299)\n",
      "3500 (5283, 8299)\n",
      "4000 (5283, 8299)\n",
      "4500 (5283, 8299)\n",
      "5000 (5283, 8299)\n",
      "start\n",
      "iterating\n",
      "0 (698, 8299)\n",
      "500 (698, 8299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 579 tn= 0 fp= 119 fn= 0\n",
      "scoring= 0.829512893982808\n",
      "start\n",
      "iterating\n",
      "0 (5764, 8823)\n",
      "500 (5764, 8823)\n",
      "1000 (5764, 8823)\n",
      "1500 (5764, 8823)\n",
      "2000 (5764, 8823)\n",
      "2500 (5764, 8823)\n",
      "3000 (5764, 8823)\n",
      "3500 (5764, 8823)\n",
      "4000 (5764, 8823)\n",
      "4500 (5764, 8823)\n",
      "5000 (5764, 8823)\n",
      "5500 (5764, 8823)\n",
      "start\n",
      "iterating\n",
      "0 (217, 8823)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 198 tn= 0 fp= 19 fn= 0\n",
      "scoring= 0.9124423963133641\n",
      "start\n",
      "iterating\n",
      "0 (5176, 8431)\n",
      "500 (5176, 8431)\n",
      "1000 (5176, 8431)\n",
      "1500 (5176, 8431)\n",
      "2000 (5176, 8431)\n",
      "2500 (5176, 8431)\n",
      "3000 (5176, 8431)\n",
      "3500 (5176, 8431)\n",
      "4000 (5176, 8431)\n",
      "4500 (5176, 8431)\n",
      "5000 (5176, 8431)\n",
      "start\n",
      "iterating\n",
      "0 (805, 8431)\n",
      "500 (805, 8431)\n",
      "tp= 625 tn= 0 fp= 180 fn= 0\n",
      "scoring= 0.7763975155279503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    wb_x_train = apply_witten_bell(x_train[it])\n",
    "    wb_x_test = apply_witten_bell(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(wb_x_train, y_train[it])\n",
    "    predicted = clf.predict(wb_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 5008 tn= 0 fp= 973 fn= 0\n",
      "scoring= 0.8373181742183581\n"
     ]
    }
   ],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Smoothing\n",
    "\n",
    "(without log-linear smoothing like Sampson, 1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "iterating\n",
      "0 (5792, 9137)\n",
      "500 (5792, 9137)\n",
      "1000 (5792, 9137)\n",
      "1500 (5792, 9137)\n",
      "2000 (5792, 9137)\n",
      "2500 (5792, 9137)\n",
      "3000 (5792, 9137)\n",
      "3500 (5792, 9137)\n",
      "4000 (5792, 9137)\n",
      "4500 (5792, 9137)\n",
      "5000 (5792, 9137)\n",
      "5500 (5792, 9137)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9127</th>\n",
       "      <th>9128</th>\n",
       "      <th>9129</th>\n",
       "      <th>9130</th>\n",
       "      <th>9131</th>\n",
       "      <th>9132</th>\n",
       "      <th>9133</th>\n",
       "      <th>9134</th>\n",
       "      <th>9135</th>\n",
       "      <th>9136</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.004836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.003950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.020198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.004831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.007706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.004835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.003511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "      <td>0.014148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.013479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.006823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "      <td>0.013947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.025204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.013502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767</th>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.004832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5769</th>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.004829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5770</th>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.006156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5772</th>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5773</th>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5774</th>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.006373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5775</th>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776</th>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5777</th>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.002850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5778</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5779</th>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5780</th>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781</th>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5782</th>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5783</th>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5784</th>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.012359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.019989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.006154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.007254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.012804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5792 rows  9137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.001095  0.001095  0.001095  0.000000  0.001095  0.001095  0.001095   \n",
       "1     0.003071  0.003071  0.003071  0.000000  0.003071  0.003071  0.003071   \n",
       "2     0.004836  0.004836  0.004836  0.004836  0.004836  0.004836  0.004836   \n",
       "3     0.003950  0.003950  0.003950  0.000000  0.003950  0.003950  0.003950   \n",
       "4     0.001314  0.001314  0.001314  0.001314  0.001314  0.001314  0.001314   \n",
       "5     0.020198  0.020198  0.020198  0.020198  0.020198  0.527473  0.020198   \n",
       "6     0.003509  0.003509  0.003509  0.003509  0.003509  0.003509  0.003509   \n",
       "7     0.004831  0.004831  0.004831  0.000000  0.004831  0.004831  0.004831   \n",
       "8     0.001753  0.001753  0.001753  0.001753  0.001753  0.001753  0.001753   \n",
       "9     0.007706  0.007706  0.007706  2.857143  0.007706  0.007706  0.007706   \n",
       "10    0.001536  0.001536  0.001536  0.001536  0.001536  0.001536  0.001536   \n",
       "11    0.004835  0.004835  0.004835  0.004835  0.004835  0.004835  0.004835   \n",
       "12    0.002632  0.002632  0.002632  0.002632  0.002632  0.002632  0.002632   \n",
       "13    0.003511  0.003511  0.003511  0.003511  0.003511  0.750000  0.003511   \n",
       "14    0.000438  0.000438  0.000438  0.000000  0.000438  0.000438  0.000438   \n",
       "15    0.014148  0.014148  0.014148  0.014148  0.014148  0.014148  0.014148   \n",
       "16    0.000438  0.000438  0.000438  0.000438  0.000438  0.000438  0.000438   \n",
       "17    0.001315  0.001315  0.001315  0.001315  0.001315  0.001315  0.001315   \n",
       "18    0.013479  0.013479  0.013479  0.013479  0.013479  0.013479  0.013479   \n",
       "19    0.006823  0.006823  0.006823  0.006823  0.006823  0.006823  0.006823   \n",
       "20    0.002411  0.002411  0.002411  0.002411  0.002411  0.002411  0.002411   \n",
       "21    0.013947  0.013947  0.013947  0.013947  0.013947  0.013947  0.013947   \n",
       "22    0.004393  0.004393  0.004393  0.004393  2.000000  0.004393  0.004393   \n",
       "23    0.002412  0.002412  0.002412  0.002412  0.002412  0.002412  0.002412   \n",
       "24    0.003510  0.003510  0.003510  0.003510  0.003510  0.003510  0.003510   \n",
       "25    0.002631  0.002631  0.002631  0.002631  0.002631  0.002631  0.002631   \n",
       "26    0.025204  0.025204  0.025204  5.000000  1.333333  0.025204  0.025204   \n",
       "27    0.006381  0.006381  0.006381  1.000000  0.006381  0.006381  0.006381   \n",
       "28    0.000876  0.000876  0.000876  0.000000  0.000876  0.000876  0.000876   \n",
       "29    0.002631  0.002631  0.002631  0.750000  0.002631  0.002631  0.002631   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5762  0.013502  0.013502  0.013502  1.176471  0.013502  0.013502  0.013502   \n",
       "5763  0.002853  0.002853  0.002853  0.002853  0.002853  0.002853  0.002853   \n",
       "5764  0.002192  0.002192  0.002192  0.900000  0.002192  0.002192  0.002192   \n",
       "5765  0.001972  0.001972  0.001972  0.001972  0.001972  0.001972  0.001972   \n",
       "5766  0.002193  0.002193  0.002193  0.002193  0.002193  0.002193  0.002193   \n",
       "5767  0.001315  0.001315  0.001315  0.001315  0.001315  0.001315  0.001315   \n",
       "5768  0.004832  0.004832  0.004832  0.004832  0.004832  0.004832  0.004832   \n",
       "5769  0.004829  0.004829  0.004829  0.004829  0.004829  0.004829  0.004829   \n",
       "5770  0.002412  0.002412  0.002412  0.000000  0.002412  0.002412  0.002412   \n",
       "5771  0.006156  0.006156  0.006156  0.006156  0.006156  0.006156  0.006156   \n",
       "5772  0.004171  0.004171  0.004171  0.004171  0.004171  0.004171  0.004171   \n",
       "5773  0.006813  0.006813  0.006813  2.000000  0.006813  0.006813  0.006813   \n",
       "5774  0.006373  0.006373  0.006373  0.000000  0.006373  0.006373  0.006373   \n",
       "5775  0.003513  0.003513  0.003513  0.003513  0.003513  0.003513  0.003513   \n",
       "5776  0.003949  0.003949  0.003949  0.003949  0.003949  0.003949  0.003949   \n",
       "5777  0.002850  0.002850  0.002850  0.002850  0.002850  0.002850  0.002850   \n",
       "5778  0.000438  0.000438  0.000438  0.000000  0.000438  0.000438  0.000438   \n",
       "5779  0.004170  0.004170  0.004170  0.000000  0.004170  0.004170  0.004170   \n",
       "5780  0.002192  0.002192  0.002192  0.002192  0.002192  0.002192  0.002192   \n",
       "5781  0.001314  0.001314  0.001314  0.001314  0.001314  0.001314  0.001314   \n",
       "5782  0.004393  0.004393  0.004393  0.004393  0.004393  0.004393  0.004393   \n",
       "5783  0.001753  0.001753  0.001753  0.001753  0.001753  0.001753  0.001753   \n",
       "5784  0.012359  0.012359  0.012359  0.000000  0.012359  0.012359  0.012359   \n",
       "5785  0.019989  0.019989  0.019989  0.019989  0.019989  0.019989  0.019989   \n",
       "5786  0.004615  0.004615  0.004615  0.004615  0.004615  0.004615  0.004615   \n",
       "5787  0.006154  0.006154  0.006154  0.006154  0.006154  0.006154  0.006154   \n",
       "5788  0.001096  0.001096  0.001096  0.001096  0.001096  0.001096  0.001096   \n",
       "5789  0.000876  0.000876  0.000876  0.000000  0.000876  0.000876  0.000876   \n",
       "5790  0.007254  0.007254  0.007254  0.007254  0.007254  0.007254  0.007254   \n",
       "5791  0.012804  0.012804  0.012804  0.517241  2.800000  0.012804  0.012804   \n",
       "\n",
       "          7         8         9     ...      9127      9128      9129  \\\n",
       "0     0.001095  0.001095  0.001095  ...  0.001095  0.001095  0.001095   \n",
       "1     0.003071  0.003071  0.003071  ...  0.003071  0.003071  0.003071   \n",
       "2     0.004836  0.004836  0.004836  ...  0.004836  0.004836  0.004836   \n",
       "3     0.003950  0.003950  0.003950  ...  0.003950  0.003950  0.003950   \n",
       "4     0.001314  0.001314  0.001314  ...  0.001314  0.001314  0.001314   \n",
       "5     0.020198  0.020198  0.020198  ...  0.020198  0.020198  0.020198   \n",
       "6     0.003509  0.003509  0.003509  ...  0.003509  0.003509  0.003509   \n",
       "7     0.004831  0.004831  0.004831  ...  0.004831  0.004831  0.004831   \n",
       "8     0.001753  0.001753  0.001753  ...  0.001753  0.001753  0.001753   \n",
       "9     0.007706  0.007706  0.007706  ...  0.007706  0.007706  0.007706   \n",
       "10    0.001536  0.001536  0.001536  ...  0.001536  0.001536  0.001536   \n",
       "11    0.004835  0.004835  0.004835  ...  0.004835  0.004835  0.004835   \n",
       "12    0.002632  0.002632  0.002632  ...  0.002632  0.002632  0.002632   \n",
       "13    0.003511  0.003511  0.003511  ...  0.003511  0.003511  0.003511   \n",
       "14    0.000438  0.000438  0.000438  ...  0.000438  0.000438  0.000438   \n",
       "15    0.014148  0.014148  0.014148  ...  0.014148  0.014148  0.014148   \n",
       "16    0.000438  0.000438  0.000438  ...  0.000438  0.000438  0.000438   \n",
       "17    0.001315  0.001315  0.001315  ...  0.001315  0.001315  0.001315   \n",
       "18    0.013479  0.013479  0.013479  ...  0.013479  0.013479  0.013479   \n",
       "19    0.006823  0.006823  0.006823  ...  0.006823  0.006823  0.006823   \n",
       "20    0.002411  0.002411  0.002411  ...  0.002411  0.002411  0.002411   \n",
       "21    0.013947  0.013947  0.013947  ...  0.013947  0.013947  0.013947   \n",
       "22    2.000000  0.004393  0.004393  ...  0.004393  0.004393  0.004393   \n",
       "23    0.002412  0.002412  0.002412  ...  0.002412  0.002412  0.002412   \n",
       "24    0.003510  0.003510  0.003510  ...  0.003510  0.003510  0.003510   \n",
       "25    0.002631  0.002631  0.002631  ...  0.002631  0.002631  0.002631   \n",
       "26    1.333333  0.025204  0.025204  ...  0.025204  0.025204  0.025204   \n",
       "27    0.006381  0.006381  0.006381  ...  0.006381  0.006381  0.006381   \n",
       "28    0.000876  0.000876  0.000876  ...  0.000876  0.000876  0.000876   \n",
       "29    0.002631  0.002631  0.002631  ...  0.002631  0.002631  0.002631   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5762  0.013502  0.013502  0.013502  ...  0.013502  0.013502  0.013502   \n",
       "5763  0.002853  0.002853  0.002853  ...  0.002853  0.002853  0.002853   \n",
       "5764  0.002192  0.002192  0.002192  ...  0.002192  0.002192  0.002192   \n",
       "5765  0.001972  0.001972  0.001972  ...  0.001972  0.001972  0.001972   \n",
       "5766  0.002193  0.002193  0.002193  ...  0.002193  0.002193  0.002193   \n",
       "5767  0.001315  0.001315  0.001315  ...  0.001315  0.001315  0.001315   \n",
       "5768  0.004832  0.004832  0.004832  ...  0.004832  0.004832  0.004832   \n",
       "5769  0.004829  0.004829  0.004829  ...  0.004829  0.004829  0.004829   \n",
       "5770  0.002412  0.002412  0.002412  ...  0.002412  0.002412  0.002412   \n",
       "5771  0.006156  0.006156  0.006156  ...  0.006156  0.006156  0.006156   \n",
       "5772  0.004171  0.004171  0.004171  ...  0.004171  0.004171  0.004171   \n",
       "5773  0.006813  0.006813  0.006813  ...  0.006813  0.006813  0.006813   \n",
       "5774  0.006373  0.006373  0.006373  ...  0.006373  0.006373  0.006373   \n",
       "5775  0.003513  0.003513  0.003513  ...  0.003513  0.003513  0.003513   \n",
       "5776  0.003949  0.003949  0.003949  ...  0.003949  0.003949  0.003949   \n",
       "5777  0.002850  0.002850  0.002850  ...  0.002850  0.002850  0.002850   \n",
       "5778  0.000438  0.000438  0.000438  ...  0.000438  0.000438  0.000438   \n",
       "5779  0.004170  0.004170  0.004170  ...  0.004170  0.004170  0.004170   \n",
       "5780  0.002192  0.002192  0.002192  ...  0.002192  0.002192  0.002192   \n",
       "5781  0.001314  0.001314  0.001314  ...  0.001314  0.001314  0.001314   \n",
       "5782  0.004393  0.004393  0.004393  ...  0.004393  0.004393  0.004393   \n",
       "5783  0.001753  0.001753  0.001753  ...  0.001753  0.001753  0.001753   \n",
       "5784  0.012359  0.012359  0.012359  ...  0.012359  0.012359  0.012359   \n",
       "5785  0.019989  0.019989  0.019989  ...  0.019989  0.019989  0.019989   \n",
       "5786  0.004615  0.004615  0.004615  ...  0.004615  0.004615  0.004615   \n",
       "5787  0.006154  0.006154  0.006154  ...  0.006154  0.006154  0.006154   \n",
       "5788  0.001096  0.001096  0.001096  ...  0.001096  0.001096  0.001096   \n",
       "5789  0.000876  0.000876  0.000876  ...  0.000876  0.000876  0.000876   \n",
       "5790  0.007254  0.007254  0.007254  ...  0.007254  0.007254  0.007254   \n",
       "5791  2.800000  0.012804  0.012804  ...  0.012804  0.012804  0.012804   \n",
       "\n",
       "          9130      9131      9132      9133      9134      9135      9136  \n",
       "0     0.001095  0.001095  0.001095  0.001095  0.001095  0.001095  0.001095  \n",
       "1     0.003071  0.003071  0.003071  0.003071  0.003071  0.003071  0.003071  \n",
       "2     0.004836  0.004836  0.004836  0.004836  0.004836  0.004836  0.004836  \n",
       "3     0.003950  0.003950  0.003950  0.003950  0.003950  0.003950  0.003950  \n",
       "4     0.001314  0.001314  0.001314  0.001314  0.001314  0.001314  0.001314  \n",
       "5     0.020198  0.020198  0.020198  0.020198  0.020198  0.020198  0.020198  \n",
       "6     0.003509  0.003509  0.003509  0.003509  0.003509  0.003509  0.003509  \n",
       "7     0.004831  0.004831  0.004831  0.004831  0.004831  0.004831  0.004831  \n",
       "8     0.001753  0.001753  0.001753  0.001753  0.001753  0.001753  0.001753  \n",
       "9     0.007706  0.007706  0.007706  0.007706  0.007706  0.007706  0.007706  \n",
       "10    0.001536  0.001536  0.001536  0.001536  0.001536  0.001536  0.001536  \n",
       "11    0.004835  0.004835  0.004835  0.004835  0.004835  0.004835  0.004835  \n",
       "12    0.002632  0.002632  0.002632  0.002632  0.002632  0.002632  0.002632  \n",
       "13    0.003511  0.003511  0.003511  0.003511  0.003511  0.003511  0.003511  \n",
       "14    0.000438  0.000438  0.000438  0.000438  0.000438  0.000438  0.000438  \n",
       "15    0.014148  0.014148  0.014148  0.014148  0.014148  0.014148  0.014148  \n",
       "16    0.000438  0.000438  0.000438  0.000438  0.000438  0.000438  0.000438  \n",
       "17    0.001315  0.001315  0.001315  0.001315  0.001315  0.001315  0.001315  \n",
       "18    0.013479  0.013479  0.013479  0.013479  0.013479  0.013479  0.013479  \n",
       "19    0.006823  0.006823  0.006823  0.006823  0.006823  0.006823  0.006823  \n",
       "20    0.002411  0.002411  0.002411  0.002411  0.002411  0.002411  0.002411  \n",
       "21    0.013947  0.013947  0.013947  0.013947  0.013947  0.013947  0.013947  \n",
       "22    0.004393  0.004393  0.004393  0.004393  0.004393  0.004393  0.004393  \n",
       "23    0.002412  0.002412  0.002412  0.002412  0.002412  0.002412  0.002412  \n",
       "24    0.003510  0.003510  0.003510  0.003510  0.003510  0.003510  0.003510  \n",
       "25    0.002631  0.002631  0.002631  0.002631  0.002631  0.002631  0.002631  \n",
       "26    0.025204  0.025204  0.025204  0.025204  0.025204  0.025204  0.025204  \n",
       "27    0.006381  0.006381  0.006381  0.006381  0.006381  0.006381  0.006381  \n",
       "28    0.000876  0.000876  0.000876  0.000876  0.000876  0.000876  0.000876  \n",
       "29    0.002631  0.002631  0.002631  0.002631  0.002631  0.002631  0.002631  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5762  0.013502  0.013502  0.013502  0.013502  0.013502  0.013502  0.013502  \n",
       "5763  0.002853  0.002853  0.002853  0.002853  0.002853  0.002853  0.002853  \n",
       "5764  0.002192  0.002192  0.002192  0.002192  0.002192  0.002192  0.002192  \n",
       "5765  0.001972  0.001972  0.001972  0.001972  0.001972  0.001972  0.001972  \n",
       "5766  0.002193  0.002193  0.002193  0.002193  0.002193  0.002193  0.002193  \n",
       "5767  0.001315  0.001315  0.001315  0.001315  0.001315  0.001315  0.001315  \n",
       "5768  0.004832  0.004832  0.004832  0.004832  0.004832  0.004832  0.004832  \n",
       "5769  0.004829  0.004829  0.004829  0.004829  0.004829  0.004829  0.004829  \n",
       "5770  0.002412  0.002412  0.002412  0.002412  0.002412  0.002412  0.002412  \n",
       "5771  0.006156  0.006156  0.006156  0.006156  0.006156  0.006156  0.006156  \n",
       "5772  0.004171  0.004171  0.004171  0.004171  0.004171  0.004171  0.004171  \n",
       "5773  0.006813  0.006813  0.006813  0.006813  0.006813  0.006813  0.006813  \n",
       "5774  0.006373  0.006373  0.006373  0.006373  0.006373  0.006373  0.006373  \n",
       "5775  0.003513  0.003513  0.003513  0.003513  0.003513  0.003513  0.003513  \n",
       "5776  0.003949  0.003949  0.003949  0.003949  0.003949  0.003949  0.003949  \n",
       "5777  0.002850  0.002850  0.002850  0.002850  0.002850  0.002850  0.002850  \n",
       "5778  0.000438  0.000438  0.000438  0.000438  0.000438  0.000438  0.000438  \n",
       "5779  0.004170  0.004170  0.473684  0.004170  0.004170  0.004170  0.004170  \n",
       "5780  0.002192  0.002192  0.002192  0.002192  0.002192  0.002192  0.002192  \n",
       "5781  0.001314  0.001314  0.001314  0.001314  0.001314  0.001314  0.001314  \n",
       "5782  0.004393  0.004393  0.004393  0.004393  0.004393  0.004393  0.004393  \n",
       "5783  0.001753  0.001753  0.001753  0.001753  0.001753  0.001753  0.001753  \n",
       "5784  0.012359  0.012359  0.012359  0.012359  0.012359  0.012359  0.012359  \n",
       "5785  0.019989  0.019989  0.019989  0.019989  0.019989  0.019989  0.019989  \n",
       "5786  0.004615  0.004615  0.004615  0.004615  0.004615  0.004615  0.004615  \n",
       "5787  0.006154  0.006154  0.006154  0.006154  0.006154  0.006154  0.006154  \n",
       "5788  0.001096  0.001096  0.001096  0.001096  0.001096  0.001096  0.001096  \n",
       "5789  0.000876  0.000876  0.000876  0.000876  0.000876  0.000876  0.000876  \n",
       "5790  0.007254  0.007254  0.007254  0.007254  0.007254  0.007254  0.007254  \n",
       "5791  0.012804  0.012804  0.012804  0.012804  0.012804  0.012804  0.012804  \n",
       "\n",
       "[5792 rows x 9137 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_good_turing(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    \n",
    "    df_x = df_x + 1 #add one as mentioned in paper\n",
    "    \n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        r_stars = {}\n",
    "        if it %500 == 0:\n",
    "            print(it, df_x.shape)\n",
    "            \n",
    "        vc = row.value_counts().to_dict()\n",
    "        for it in range(0, max(vc.keys())):\n",
    "            if it not in vc:\n",
    "                vc[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        for r in sorted(vc):\n",
    "            Nr_plus_1 = 0 if r+1 not in vc else vc[r+1]\n",
    "            Nr = vc[r] if r in vc else [vc[r_] for r_ in range(r, 0) if vc[r_1] > 0][0] # take next smallest value\n",
    "            Nr = Nr if Nr > 0 else 1\n",
    "            r_star = (r + 1) * (Nr_plus_1/Nr)\n",
    "            #print(\"r*\", r, Nr, Nr_plus_1, (Nr_plus_1/Nr) , r_star)\n",
    "            r_stars[r] = r_star\n",
    "        #print(vc, r_stars)\n",
    "        for it in range(0, max(r_stars.keys())):\n",
    "            if it not in r_stars:\n",
    "                r_stars[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        res.append(row.map(lambda n: r_stars[n]))\n",
    "      \n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "text = [[1,0,1,0,1], [2,1,0,1,2], [5,0,0,1,0], [2,3,0,0,0]]\n",
    "apply_good_turing(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "iterating\n",
      "0 (5792, 9137)\n",
      "500 (5792, 9137)\n",
      "1000 (5792, 9137)\n",
      "1500 (5792, 9137)\n",
      "2000 (5792, 9137)\n",
      "2500 (5792, 9137)\n",
      "3000 (5792, 9137)\n",
      "3500 (5792, 9137)\n",
      "4000 (5792, 9137)\n",
      "4500 (5792, 9137)\n",
      "5000 (5792, 9137)\n",
      "5500 (5792, 9137)\n",
      "start\n",
      "iterating\n",
      "0 (189, 9137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 137 tn= 17 fp= 16 fn= 19\n",
      "scoring= 0.8148148148148148\n",
      "start\n",
      "iterating\n",
      "0 (4591, 7935)\n",
      "500 (4591, 7935)\n",
      "1000 (4591, 7935)\n",
      "1500 (4591, 7935)\n",
      "2000 (4591, 7935)\n",
      "2500 (4591, 7935)\n",
      "3000 (4591, 7935)\n",
      "3500 (4591, 7935)\n",
      "4000 (4591, 7935)\n",
      "4500 (4591, 7935)\n",
      "start\n",
      "iterating\n",
      "0 (1390, 7935)\n",
      "500 (1390, 7935)\n",
      "1000 (1390, 7935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 1110 tn= 90 fp= 119 fn= 71\n",
      "scoring= 0.8633093525179856\n",
      "start\n",
      "iterating\n",
      "0 (4960, 8068)\n",
      "500 (4960, 8068)\n",
      "1000 (4960, 8068)\n",
      "1500 (4960, 8068)\n",
      "2000 (4960, 8068)\n",
      "2500 (4960, 8068)\n",
      "3000 (4960, 8068)\n",
      "3500 (4960, 8068)\n",
      "4000 (4960, 8068)\n",
      "4500 (4960, 8068)\n",
      "start\n",
      "iterating\n",
      "0 (1021, 8068)\n",
      "500 (1021, 8068)\n",
      "1000 (1021, 8068)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 774 tn= 80 fp= 107 fn= 60\n",
      "scoring= 0.8364348677766895\n",
      "start\n",
      "iterating\n",
      "0 (4320, 7986)\n",
      "500 (4320, 7986)\n",
      "1000 (4320, 7986)\n",
      "1500 (4320, 7986)\n",
      "2000 (4320, 7986)\n",
      "2500 (4320, 7986)\n",
      "3000 (4320, 7986)\n",
      "3500 (4320, 7986)\n",
      "4000 (4320, 7986)\n",
      "start\n",
      "iterating\n",
      "0 (1661, 7986)\n",
      "500 (1661, 7986)\n",
      "1000 (1661, 7986)\n",
      "1500 (1661, 7986)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 1299 tn= 101 fp= 125 fn= 136\n",
      "scoring= 0.8428657435279951\n",
      "start\n",
      "iterating\n",
      "0 (5283, 8299)\n",
      "500 (5283, 8299)\n",
      "1000 (5283, 8299)\n",
      "1500 (5283, 8299)\n",
      "2000 (5283, 8299)\n",
      "2500 (5283, 8299)\n",
      "3000 (5283, 8299)\n",
      "3500 (5283, 8299)\n",
      "4000 (5283, 8299)\n",
      "4500 (5283, 8299)\n",
      "5000 (5283, 8299)\n",
      "start\n",
      "iterating\n",
      "0 (698, 8299)\n",
      "500 (698, 8299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 509 tn= 52 fp= 67 fn= 70\n",
      "scoring= 0.8037249283667621\n",
      "start\n",
      "iterating\n",
      "0 (5764, 8823)\n",
      "500 (5764, 8823)\n",
      "1000 (5764, 8823)\n",
      "1500 (5764, 8823)\n",
      "2000 (5764, 8823)\n",
      "2500 (5764, 8823)\n",
      "3000 (5764, 8823)\n",
      "3500 (5764, 8823)\n",
      "4000 (5764, 8823)\n",
      "4500 (5764, 8823)\n",
      "5000 (5764, 8823)\n",
      "5500 (5764, 8823)\n",
      "start\n",
      "iterating\n",
      "0 (217, 8823)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 181 tn= 9 fp= 10 fn= 17\n",
      "scoring= 0.8755760368663594\n",
      "start\n",
      "iterating\n",
      "0 (5176, 8431)\n",
      "500 (5176, 8431)\n",
      "1000 (5176, 8431)\n",
      "1500 (5176, 8431)\n",
      "2000 (5176, 8431)\n",
      "2500 (5176, 8431)\n",
      "3000 (5176, 8431)\n",
      "3500 (5176, 8431)\n",
      "4000 (5176, 8431)\n",
      "4500 (5176, 8431)\n",
      "5000 (5176, 8431)\n",
      "start\n",
      "iterating\n",
      "0 (805, 8431)\n",
      "500 (805, 8431)\n",
      "tp= 559 tn= 74 fp= 106 fn= 66\n",
      "scoring= 0.7863354037267081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    gt_x_train = apply_good_turing(x_train[it])\n",
    "    gt_x_test = apply_good_turing(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(gt_x_train, y_train[it])\n",
    "    predicted = clf.predict(gt_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 4569 tn= 423 fp= 550 fn= 439\n",
      "scoring= 0.8346430362815582\n"
     ]
    }
   ],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(tokens, n):\n",
    "    res = []\n",
    "    for it in range(0, len(tokens) -(n-1)):\n",
    "        res.append(\" \". join(tokens[it: it +n]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "5792 189 5792 189\n",
      "4591 1390 4591 1390\n",
      "4960 1021 4960 1021\n",
      "4320 1661 4320 1661\n",
      "5283 698 5283 698\n",
      "5764 217 5764 217\n",
      "5176 805 5176 805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "datasets = load_dataset(dataset_paths[0])\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    df_train[\"tokens\"] = df_train.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    df_test[\"tokens\"] = df_test.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 148 tn= 25 fp= 8 fn= 8\n",
      "scoring= 0.9153439153439153\n",
      "tp= 1137 tn= 142 fp= 67 fn= 44\n",
      "scoring= 0.9201438848920863\n",
      "tp= 812 tn= 107 fp= 80 fn= 22\n",
      "scoring= 0.9000979431929481\n",
      "tp= 1419 tn= 128 fp= 98 fn= 16\n",
      "scoring= 0.9313666465984347\n",
      "tp= 566 tn= 82 fp= 37 fn= 13\n",
      "scoring= 0.9283667621776505\n",
      "tp= 191 tn= 13 fp= 6 fn= 7\n",
      "scoring= 0.9400921658986175\n",
      "tp= 622 tn= 106 fp= 74 fn= 3\n",
      "scoring= 0.9043478260869565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 813 tn= 957 fp= 163 fn= 307\n",
      "scoring= 0.7901785714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(dataset_paths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n",
      "4032 448 4032 448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 208 tn= 218 fp= 6 fn= 16\n",
      "scoring= 0.9508928571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 221 tn= 221 fp= 3 fn= 3\n",
      "scoring= 0.9866071428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 217 tn= 213 fp= 11 fn= 7\n",
      "scoring= 0.9598214285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 216 tn= 217 fp= 7 fn= 8\n",
      "scoring= 0.9665178571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 213 tn= 218 fp= 6 fn= 11\n",
      "scoring= 0.9620535714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 212 tn= 213 fp= 11 fn= 12\n",
      "scoring= 0.9486607142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 214 tn= 214 fp= 10 fn= 10\n",
      "scoring= 0.9553571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 214 tn= 216 fp= 8 fn= 10\n",
      "scoring= 0.9598214285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 217 tn= 218 fp= 6 fn= 7\n",
      "scoring= 0.9709821428571429\n",
      "tp= 213 tn= 214 fp= 10 fn= 11\n",
      "scoring= 0.953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 2145 tn= 2162 fp= 78 fn= 95\n",
      "scoring= 0.9613839285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 202 tn= 217 fp= 7 fn= 22\n",
      "scoring= 0.9352678571428571\n",
      "tp= 211 tn= 218 fp= 6 fn= 13\n",
      "scoring= 0.9575892857142857\n",
      "tp= 214 tn= 215 fp= 9 fn= 10\n",
      "scoring= 0.9575892857142857\n",
      "tp= 210 tn= 219 fp= 5 fn= 14\n",
      "scoring= 0.9575892857142857\n",
      "tp= 214 tn= 218 fp= 6 fn= 10\n",
      "scoring= 0.9642857142857143\n",
      "tp= 210 tn= 211 fp= 13 fn= 14\n",
      "scoring= 0.9397321428571429\n",
      "tp= 203 tn= 212 fp= 12 fn= 21\n",
      "scoring= 0.9263392857142857\n",
      "tp= 210 tn= 219 fp= 5 fn= 14\n",
      "scoring= 0.9575892857142857\n",
      "tp= 212 tn= 221 fp= 3 fn= 12\n",
      "scoring= 0.9665178571428571\n",
      "tp= 209 tn= 212 fp= 12 fn= 15\n",
      "scoring= 0.9397321428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 2095 tn= 2162 fp= 78 fn= 145\n",
      "scoring= 0.9502232142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Witten Bell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_witten_bell(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        \n",
    "        if it %500 == 0:\n",
    "            print(it, x.shape)\n",
    "        N = sum(row)\n",
    "        #print(\"N\", N)\n",
    "        M = sum([1 for x in row if x != 0])\n",
    "        #print(\"M\", M)\n",
    "        row = row/row.sum()\n",
    "        nval = 1/(N+M)\n",
    "        #print(\"applying\", nval)\n",
    "        row = row.replace(0, nval)\n",
    "        #print(\"set value\")\n",
    "        res.append(row)\n",
    "        #print(\"ready with row\")\n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "iterating\n",
      "0 (4032, 7418)\n",
      "500 (4032, 7418)\n",
      "1000 (4032, 7418)\n",
      "1500 (4032, 7418)\n",
      "2000 (4032, 7418)\n",
      "2500 (4032, 7418)\n",
      "3000 (4032, 7418)\n",
      "3500 (4032, 7418)\n",
      "4000 (4032, 7418)\n",
      "start\n",
      "iterating\n",
      "0 (448, 7418)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 160 tn= 189 fp= 35 fn= 64\n",
      "scoring= 0.7790178571428571\n",
      "start\n",
      "iterating\n",
      "0 (4032, 7408)\n",
      "500 (4032, 7408)\n",
      "1000 (4032, 7408)\n",
      "1500 (4032, 7408)\n",
      "2000 (4032, 7408)\n",
      "2500 (4032, 7408)\n",
      "3000 (4032, 7408)\n",
      "3500 (4032, 7408)\n",
      "4000 (4032, 7408)\n",
      "start\n",
      "iterating\n",
      "0 (448, 7408)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 168 tn= 189 fp= 35 fn= 56\n",
      "scoring= 0.796875\n",
      "start\n",
      "iterating\n",
      "0 (4032, 7376)\n",
      "500 (4032, 7376)\n",
      "1000 (4032, 7376)\n",
      "1500 (4032, 7376)\n",
      "2000 (4032, 7376)\n",
      "2500 (4032, 7376)\n",
      "3000 (4032, 7376)\n",
      "3500 (4032, 7376)\n",
      "4000 (4032, 7376)\n",
      "start\n",
      "iterating\n",
      "0 (448, 7376)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 165 tn= 192 fp= 32 fn= 59\n",
      "scoring= 0.796875\n",
      "start\n",
      "iterating\n",
      "0 (4032, 7410)\n",
      "500 (4032, 7410)\n",
      "1000 (4032, 7410)\n",
      "1500 (4032, 7410)\n",
      "2000 (4032, 7410)\n",
      "2500 (4032, 7410)\n",
      "3000 (4032, 7410)\n",
      "3500 (4032, 7410)\n",
      "4000 (4032, 7410)\n",
      "start\n",
      "iterating\n",
      "0 (448, 7410)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 161 tn= 188 fp= 36 fn= 63\n",
      "scoring= 0.7790178571428571\n",
      "start\n",
      "iterating\n",
      "0 (4032, 7436)\n",
      "500 (4032, 7436)\n",
      "1000 (4032, 7436)\n",
      "1500 (4032, 7436)\n",
      "2000 (4032, 7436)\n",
      "2500 (4032, 7436)\n",
      "3000 (4032, 7436)\n",
      "3500 (4032, 7436)\n",
      "4000 (4032, 7436)\n",
      "start\n",
      "iterating\n",
      "0 (448, 7436)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp= 159 tn= 199 fp= 25 fn= 65\n",
      "scoring= 0.7991071428571429\n",
      "start\n",
      "iterating\n",
      "0 (4032, 7366)\n",
      "500 (4032, 7366)\n",
      "1000 (4032, 7366)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ce8125ab2585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwb_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_witten_bell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mwb_x_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_witten_bell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-240553efcaee>\u001b[0m in \u001b[0;36mapply_witten_bell\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(\"M\", M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(\"applying\", nval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         return construct_result(left, result,\n\u001b[0;32m-> 1585\u001b[0;31m                                 index=left.index, name=res_name, dtype=None)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_construct_result\u001b[0;34m(left, result, index, name, dtype)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0menough\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mstill\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0moverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m     \"\"\"\n\u001b[0;32m-> 1474\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    262\u001b[0m                                       raise_cast_failure=True)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3086\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3088\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_block_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDatetimeTZBlock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mget_block_type\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m   3058\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3059\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeTZBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3060\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_interval_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_period_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3061\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectValuesExtensionBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3062\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_interval_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIntervalDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/dtypes/dtypes.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntervalDtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    wb_x_train = apply_witten_bell(x_train[it])\n",
    "    wb_x_test = apply_witten_bell(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(wb_x_train, y_train[it])\n",
    "    predicted = clf.predict(wb_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Smoothing\n",
    "\n",
    "(without log-linear smoothing like Sampson, 1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_good_turing(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    \n",
    "    df_x = df_x + 1 #add one as mentioned in paper\n",
    "    \n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        r_stars = {}\n",
    "        if it %500 == 0:\n",
    "            print(it, df_x.shape)\n",
    "            \n",
    "        vc = row.value_counts().to_dict()\n",
    "        for it in range(0, max(vc.keys())):\n",
    "            if it not in vc:\n",
    "                vc[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        for r in sorted(vc):\n",
    "            Nr_plus_1 = 0 if r+1 not in vc else vc[r+1]\n",
    "            Nr = vc[r] if r in vc else [vc[r_] for r_ in range(r, 0) if vc[r_1] > 0][0] # take next smallest value\n",
    "            Nr = Nr if Nr > 0 else 1\n",
    "            r_star = (r + 1) * (Nr_plus_1/Nr)\n",
    "            #print(\"r*\", r, Nr, Nr_plus_1, (Nr_plus_1/Nr) , r_star)\n",
    "            r_stars[r] = r_star\n",
    "        #print(vc, r_stars)\n",
    "        for it in range(0, max(r_stars.keys())):\n",
    "            if it not in r_stars:\n",
    "                r_stars[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        res.append(row.map(lambda n: r_stars[n]))\n",
    "      \n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "text = [[1,0,1,0,1], [2,1,0,1,2], [5,0,0,1,0], [2,3,0,0,0]]\n",
    "apply_good_turing(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    gt_x_train = apply_good_turing(x_train[it])\n",
    "    gt_x_test = apply_good_turing(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(gt_x_train, y_train[it])\n",
    "    predicted = clf.predict(gt_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(tokens, n):\n",
    "    res = []\n",
    "    for it in range(0, len(tokens) -(n-1)):\n",
    "        res.append(\" \". join(tokens[it: it +n]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "datasets = load_dataset(dataset_paths[1])\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    df_train[\"tokens\"] = df_train.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    df_test[\"tokens\"] = df_test.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
