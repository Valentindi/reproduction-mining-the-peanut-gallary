{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import enumerate, len\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [ \"../data/df_test_1.csv\", \"../data/df_test_2.csv\"]\n",
    "ngrams = [2,3]\n",
    "repl_prod_name = [True, False]\n",
    "tokens = [\"common_tokens\", \"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    train_idxes = df.test_idx.unique()\n",
    "    print(train_idxes)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for train_idx in train_idxes:\n",
    "        mask = df.test_idx == train_idx\n",
    "\n",
    "        df_train = pd.read_csv(path)\n",
    "        df_test = copy(df_train)\n",
    "\n",
    "        df_train = df_train[~mask]\n",
    "        df_test = df_test[mask]\n",
    "\n",
    "        res += [(df_train, df_test)]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(dataset_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Witten Bell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_witten_bell(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        \n",
    "        if it %500 == 0:\n",
    "            print(it, x.shape)\n",
    "        N = sum(row)\n",
    "        #print(\"N\", N)\n",
    "        M = sum([1 for x in row if x != 0])\n",
    "        #print(\"M\", M)\n",
    "        row = row/row.sum()\n",
    "        nval = 1/(N+M)\n",
    "        #print(\"applying\", nval)\n",
    "        row = row.replace(0, nval)\n",
    "        #print(\"set value\")\n",
    "        res.append(row)\n",
    "        #print(\"ready with row\")\n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    wb_x_train = apply_witten_bell(x_train[it])\n",
    "    wb_x_test = apply_witten_bell(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(wb_x_train, y_train[it])\n",
    "    predicted = clf.predict(wb_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Smoothing\n",
    "\n",
    "(without log-linear smoothing like Sampson, 1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_good_turing(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    \n",
    "    df_x = df_x + 1 #add one as mentioned in paper\n",
    "    \n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        r_stars = {}\n",
    "        if it %500 == 0:\n",
    "            print(it, df_x.shape)\n",
    "            \n",
    "        vc = row.value_counts().to_dict()\n",
    "        for it in range(0, max(vc.keys())):\n",
    "            if it not in vc:\n",
    "                vc[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        for r in sorted(vc):\n",
    "            Nr_plus_1 = 0 if r+1 not in vc else vc[r+1]\n",
    "            Nr = vc[r] if r in vc else [vc[r_] for r_ in range(r, 0) if vc[r_1] > 0][0] # take next smallest value\n",
    "            Nr = Nr if Nr > 0 else 1\n",
    "            r_star = (r + 1) * (Nr_plus_1/Nr)\n",
    "            #print(\"r*\", r, Nr, Nr_plus_1, (Nr_plus_1/Nr) , r_star)\n",
    "            r_stars[r] = r_star\n",
    "        #print(vc, r_stars)\n",
    "        for it in range(0, max(r_stars.keys())):\n",
    "            if it not in r_stars:\n",
    "                r_stars[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        res.append(row.map(lambda n: r_stars[n]))\n",
    "      \n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "text = [[1,0,1,0,1], [2,1,0,1,2], [5,0,0,1,0], [2,3,0,0,0]]\n",
    "apply_good_turing(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    gt_x_train = apply_good_turing(x_train[it])\n",
    "    gt_x_test = apply_good_turing(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(gt_x_train, y_train[it])\n",
    "    predicted = clf.predict(gt_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(tokens, n):\n",
    "    res = []\n",
    "    for it in range(0, len(tokens) -(n-1)):\n",
    "        res.append(\" \". join(tokens[it: it +n]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "datasets = load_dataset(dataset_paths[0])\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    df_train[\"tokens\"] = df_train.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    df_test[\"tokens\"] = df_test.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(dataset_paths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Witten Bell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_witten_bell(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        \n",
    "        if it %500 == 0:\n",
    "            print(it, x.shape)\n",
    "        N = sum(row)\n",
    "        #print(\"N\", N)\n",
    "        M = sum([1 for x in row if x != 0])\n",
    "        #print(\"M\", M)\n",
    "        row = row/row.sum()\n",
    "        nval = 1/(N+M)\n",
    "        #print(\"applying\", nval)\n",
    "        row = row.replace(0, nval)\n",
    "        #print(\"set value\")\n",
    "        res.append(row)\n",
    "        #print(\"ready with row\")\n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    wb_x_train = apply_witten_bell(x_train[it])\n",
    "    wb_x_test = apply_witten_bell(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(wb_x_train, y_train[it])\n",
    "    predicted = clf.predict(wb_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Smoothing\n",
    "\n",
    "(without log-linear smoothing like Sampson, 1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_good_turing(x):\n",
    "    print(\"start\")\n",
    "    df_x = pd.DataFrame(x)\n",
    "    \n",
    "    df_x = df_x + 1 #add one as mentioned in paper\n",
    "    \n",
    "    print(\"iterating\")\n",
    "    res = []\n",
    "\n",
    "    for it, row in df_x.iterrows():\n",
    "        r_stars = {}\n",
    "        if it %500 == 0:\n",
    "            print(it, df_x.shape)\n",
    "            \n",
    "        vc = row.value_counts().to_dict()\n",
    "        for it in range(0, max(vc.keys())):\n",
    "            if it not in vc:\n",
    "                vc[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        for r in sorted(vc):\n",
    "            Nr_plus_1 = 0 if r+1 not in vc else vc[r+1]\n",
    "            Nr = vc[r] if r in vc else [vc[r_] for r_ in range(r, 0) if vc[r_1] > 0][0] # take next smallest value\n",
    "            Nr = Nr if Nr > 0 else 1\n",
    "            r_star = (r + 1) * (Nr_plus_1/Nr)\n",
    "            #print(\"r*\", r, Nr, Nr_plus_1, (Nr_plus_1/Nr) , r_star)\n",
    "            r_stars[r] = r_star\n",
    "        #print(vc, r_stars)\n",
    "        for it in range(0, max(r_stars.keys())):\n",
    "            if it not in r_stars:\n",
    "                r_stars[it]= 0\n",
    "        #print(vc)\n",
    "        \n",
    "        res.append(row.map(lambda n: r_stars[n]))\n",
    "      \n",
    "    df_res = pd.DataFrame(res)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "text = [[1,0,1,0,1], [2,1,0,1,2], [5,0,0,1,0], [2,3,0,0,0]]\n",
    "apply_good_turing(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    gt_x_train = apply_good_turing(x_train[it])\n",
    "    gt_x_test = apply_good_turing(x_test[it])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(gt_x_train, y_train[it])\n",
    "    predicted = clf.predict(gt_x_test)\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(tokens, n):\n",
    "    res = []\n",
    "    for it in range(0, len(tokens) -(n-1)):\n",
    "        res.append(\" \". join(tokens[it: it +n]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "datasets = load_dataset(dataset_paths[1])\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=literal_eval)\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_train = dataset[0]\n",
    "    df_test = dataset[1]\n",
    "    df_train[\"tokens\"] = df_train.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    df_test[\"tokens\"] = df_test.tokens.map(lambda tokens: str(create_n_grams(literal_eval(tokens), 2)))\n",
    "    \n",
    "    temp_train = vectorizer.fit_transform(df_train[\"tokens\"])\n",
    "    x_train.append(temp_train.toarray())\n",
    "    y_train.append(df_train.bool_rating.tolist())\n",
    "\n",
    "    temp_test = vectorizer.transform(df_test[\"tokens\"])\n",
    "    x_test.append(temp_test.toarray())\n",
    "    y_test.append(df_test.bool_rating.tolist())\n",
    "\n",
    "    print(len(x_train[-1]), len(x_test[-1]), len(y_train[-1]), len(y_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "tps, tns, fps, fns = [],[],[],[]\n",
    "\n",
    "for it,_ in enumerate(x_train):\n",
    "    clf.fit(x_train[it], y_train[it])\n",
    "    predicted = clf.predict(x_test[it])\n",
    "\n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    for itx, x in enumerate(predicted):\n",
    "        if x and y_test[it][itx]:\n",
    "            tp += 1\n",
    "        elif x and not y_test[it][itx]:\n",
    "            fp += 1\n",
    "        elif not x and y_test[it][itx]:\n",
    "            fn += 1\n",
    "        elif not x and not y_test[it][itx]:\n",
    "            tn += 1\n",
    "\n",
    "    print(\"tp=\", tp,\"tn=\", tn,\"fp=\", fp,\"fn=\", fn)\n",
    "    print(\"scoring=\", sum([tp, tn])/sum([tp, tn, fp, fn]))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    tns.append(tn)\n",
    "    fns.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp=\", sum(tps),\"tn=\", sum(tns),\"fp=\", sum(fps),\"fn=\", sum(fns))\n",
    "print(\"scoring=\", sum([sum(tps), sum(tns)])/sum([sum(tps), sum(tns), sum(fps), sum(fns)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
